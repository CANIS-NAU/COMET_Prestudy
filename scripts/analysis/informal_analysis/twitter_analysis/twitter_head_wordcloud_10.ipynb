{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate top-ten tokens for each cluster (Twitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization work\n",
    "import pandas as pd\n",
    "\n",
    "filename =\"/home/quinton/Documents/COMET_data/twitter_comet/Twts1722-Labeled-k5k10-Cos-Euc-METADATA.csv\" #input(\"Enter the file path of the Twitter data file\")\n",
    "cluster_count = \"5\"\n",
    "twitter_data = pd.read_csv(filename)\n",
    "twitter_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we need to sort which posts correspond to which clusters (consider, one post can belong to multiple clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_data = twitter_data[[\"id\", \"clntxt\", cluster_count]]\n",
    "\n",
    "reduced_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to tokenize text\n",
    "from pprint import pprint\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import nltk.tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "custom_filter_words = [\"nan\", \"hey\", \"baj\", \"wowway\", \"though\", \"even\", \"gaye\", 'u', \"guys\" ]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "token_maker = nltk.tokenize.TweetTokenizer()\n",
    "\n",
    "\n",
    "def df_tokenize(text):\n",
    "    translate_table = dict((ord(char), None) for char in string.punctuation) # Remove punctuation\n",
    "    tweet_text = text.to_string(index=False).translate(translate_table).lower()\n",
    "    tokens = token_maker.tokenize(tweet_text)\n",
    "\n",
    "    # remove extra stop words\n",
    "    filtered_tokens = [w for w in tokens if w.lower() not in stop_words]\n",
    "    filtered_tokens = [w for w in filtered_tokens if w.lower() not in custom_filter_words]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "sorted_tokens = {}\n",
    "\n",
    "# operate one cluster at a time\n",
    "for iter in range(int(cluster_count)):\n",
    "    cluster_n = reduced_data[reduced_data[cluster_count] == int(iter)]\n",
    "    # Tokenize text in every column\n",
    "    sorted_tokens[f\"cluster_{iter}\"] = (\n",
    "        cluster_n[[\"clntxt\"]]\n",
    "        .apply(\n",
    "            df_tokenize,\n",
    "            axis=1,\n",
    "        )\n",
    "        .explode()\n",
    "        .reset_index(drop=True)\n",
    "        .value_counts()\n",
    "        .rename_axis(\"token\")\n",
    "        .reset_index(name=\"frequency\")\n",
    "        .sort_values([\"frequency\"], ascending=False)\n",
    "        #.to_csv(f\"~/Desktop/cluster_word_freq_clstr{iter}.csv\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "for iter, key in enumerate(sorted_tokens.keys()):\n",
    "#     print(key)\n",
    "#     pprint(sorted_tokens[key].head(10)) # Prints the top 10 tokens table\n",
    "#     print('\\n\\n')\n",
    "\n",
    "    wordcloud = WordCloud(width=800, height = 800, background_color='white').generate(\" \".join(sorted_tokens[key][\"token\"].tolist()))\n",
    "    plt.figure(figsize=(8,8), facecolor=None)\n",
    "    plt.title(f\"Cluster {iter}\")\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('comet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f4b5714a7c56dd44b8cb5ff471a2434f89515e6c0daec40bb71d43c5b30bc0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
